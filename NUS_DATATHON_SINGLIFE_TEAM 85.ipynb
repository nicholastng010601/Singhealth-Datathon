{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas \n",
    "#%pip install matplotlib\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME \n",
    "# %pip install joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Data Cleaning:</u>\n",
    "\n",
    "Firstly, all columns with zero or only one unique value across all observations were discarded, as such variables will not serve as relevant predictors for the target column.  \n",
    "\n",
    "For categorical data such as race_desc and annual_income_desc, one-hot encoding was employed to create dummy columns for each category in order for upcoming modeling tasks to utilize the information. Data involving dates such as min_occ_data and cltdob_fix were also manipulated to represent these variables in years and age, numerical values that can be easily used in modeling. \n",
    "\n",
    "Columns representing the various insurance product metrics and the unique policy identifiers were grouped first by the product metric (ape_, sumins_, prepaid_*) and then by the type of policy (gi, ltc, grp, inv, lh) to obtain a more generalized interpretation of the inclinations for each individual, appending the mean amount of each subcategory as new columns. This is also due to how the various insurance product metrics were encoded by unique policy identifiers, of which we had no access to the information of the exact insurance policies. Hence, no real insight can be derived. The same categorisation approach was employed for the purchase and lapse metrics such as f_ever_bought_, n_months_last_bought_, lapse_ape_, n_months_since_lapse_. Grouping similar information into meaningful categories will help the model’s performance and prevent the noise from each unique policy from overly influencing the model’s prediction and accuracy. \n",
    "\n",
    "We replaced the NaN values in categorical predictors such as flg_standard, flg_is_borderline_standard etc with their respective mean values to minimize the impact and any noise of these missing values on the model. \n",
    "\n",
    "Next, we also removed the columns hh_size_est,  ctrycode_desc and clntnum as there was already a similar column (hh_size_est) with higher precision, whereas ctrycode_desc and clntum were nominal categorical variables with no intrinsic meaning and hence yield no value in either data visualization or prediction. \n",
    "\n",
    "We also observed that the target column (f_purchase_lh) contained values of 1 and NaN, and replaced the NaN values with 0, indicating that these individuals did not purchase life / health insurance in the next 3 months.\n",
    "\n",
    "There were NaN values in the target column (f_purchase_lh) and replaced them with 0s. After transforming the columns aforementioned, we applied a MinMaxScaler to scale the different features to a comparable range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Data visualisation:</u>\n",
    "\n",
    "Observing the recency_lapse variable boxplots between customers who purchased life insurance products within the next three months and those who did not, a large proportion of customers who purchased generally tended to have more recent lapses compared to customers who did not purchase. It could suggest that recent lapses suggest that the customer is still active and hence will be more likely to purchase insurance, compared to customers who have lapsed on their payment for an extended period of time and are more likely to be inactive. \n",
    "![boxplot](./images/boxplot.jpeg) \n",
    "\n",
    "The bar plot for customers on whether they have valid direct mailing addresses appears to be a good indicator of the target, too. An extremely significant majority of customers who either did not provide a valid mailing address or any mailing address (as indicated by the grey bar) all did not purchase life or health insurance in the next 3 months, which could be understood as them not anticipating or hoping for any follow-ups with regards to the policy and hence do not warrant further effort from the insurance agents. \n",
    "![is_valid_dm](./images/is_valid_dm.jpeg) \n",
    "\n",
    "\n",
    "<u>Feature selection:</u>\n",
    "\n",
    "For feature selection, we utilized SelectKBest as part of the feature selection module sklearn.feature.selection to select the top k features that are most relevant with regards to the target variable. The scoring function we used was mutual_info_classif, which is commonly used for classification tasks with both categorical and numerical variables. In this context, it quantifies the dependency between the feature and the target, assigning a higher score to features that are more informative to the target variable. From our work, we found that roughly 16 features were required to yield the best results for model prediction through cross validation with 5 splits. \n",
    "\n",
    "<u>Modeling:</u>\n",
    "\n",
    "From the data visualization figure, we noticed that an overwhelming 96.1% of the target variable f_purchase_lh were NaNs (1s: 710, NaNs: 17282), indicating that almost all of the customers surveyed would not purchase life or health insurance products within the next three months. In a heavily imbalanced dataset like this, machine learning models might be biased towards the majority target classes, leading to wildly inaccurate predictions. Thus, we tackled this issue by first upsampling the minority sample, in which case is the 1s of f_purchase_lh, by 10% and thereafter utilizing Synthetic Minority Oversampling Technique (SMOTE) to artificially generate more synthetic data via interpolation between existing minority samples. This led to a much more balanced dataset suitable for training. Downsampling was not chosen due to its possibility of removing important information unknowingly in the process. \n",
    "\n",
    "<u>Evaluation of model performance:</u>\n",
    "\n",
    "We made use of the train_test_split function to test our data, utilising it the standard 80-20 split. We considered the use of different models, both linear and nonlinear, such as LogisticRegression, SVM, MLPClassifier and RandomForestClassifier. Ultimately, we decided on utilising LinearSVC. The justifications were mainly due to the presence of regularisation parameter to ensure there is no overfitting. We also performed hyperparameters tuning by running cross validation, with 5 folds, on different values for the penalty function, C, of the SVM.SVC model. After this, we made use of the classification report to evaluate our model using different metrics such as precision, recall to calculate our F1 score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLPClassifier_confusion_matrix](./images/MLPClassifier_confusion_matrix.jpg) \n",
    "![LinearSVC_confusion_matrix](./images/LinearSVC_confusion_matrix.jpg) \n",
    "![DecisionTree_confusion_matrix](./images/DecisionTree_confusion_matrix.jpg) \n",
    "\n",
    "![LogisticRegression_confusion_matrix](./images/LogisticRegression_confusion_matrix.jpeg) \n",
    "\n",
    "![LogisticRegression_classification_report](./images/LogisticRegression_classification_report.jpeg) \n",
    "\n",
    "![LinearSVC_classification_report](./images/LinearSVC_classification_report.jpeg) \n",
    "\n",
    "To decide which model was best suited for our dataset, we trained various models to see which was able to perform best with our given dataset. With reference to the confusion matrix, MLP Classifier with 100 layers gave us the best true positive results with 2767 true positives, followed by linear svc at 2887. As for true negatives, logistic regression gave the best true negatives at 89, followed by linear svc at 84. After careful consideration, we chose to utilise linear SVC as it was consistent at identifying both positive and negative results, giving a weighted average f1 score of 0.88 as per the generated classification report, which was higher than the other models we tried. Furthermore, it gave a relatively efficient run time, taking just 20 seconds to train as compared to the MLP Classifier of 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from decimal import Decimal\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def clean_data(input_data, isTraining):\n",
    "    test_df = input_data\n",
    "\n",
    "    # cols to replace nan to mean\n",
    "    columns_to_replace = [\n",
    "        'flg_substandard', 'flg_is_borderline_standard', 'flg_is_revised_term',\n",
    "        'flg_is_rental_flat', 'flg_has_health_claim', 'flg_has_life_claim', \n",
    "        'flg_gi_claim', 'flg_is_proposal', 'flg_with_preauthorisation',\n",
    "        'flg_is_returned_mail', 'is_consent_to_mail', 'is_consent_to_email',\n",
    "        'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm', 'is_valid_email',\n",
    "        'is_housewife_retiree', 'is_sg_pr', 'is_class_1_2',\n",
    "        'is_dependent_in_at_least_1_policy', 'f_ever_declined_la',\n",
    "        'flg_latest_being_lapse', 'flg_latest_being_cancel', 'f_hold_839f8a',\n",
    "        'f_hold_e22a6a', 'f_hold_d0adeb', 'f_hold_c4bda5', 'f_hold_ltc',\n",
    "        'f_hold_507c37', 'f_hold_gi', 'f_ever_bought_ltc_1280bf',\n",
    "        'f_ever_bought_grp_6fc3e6', 'f_ever_bought_grp_de05ae',\n",
    "        'f_ever_bought_inv_dcd836', 'f_ever_bought_grp_945b5a',\n",
    "        'f_ever_bought_grp_6a5788', 'f_ever_bought_ltc_43b9d5',\n",
    "        'f_ever_bought_grp_9cdedf', 'f_ever_bought_lh_d0adeb',\n",
    "        'f_ever_bought_grp_1581d7', 'f_ever_bought_grp_22decf',\n",
    "        'f_ever_bought_lh_507c37', 'f_ever_bought_lh_839f8a',\n",
    "        'f_ever_bought_inv_e9f316', 'f_ever_bought_grp_caa6ff',\n",
    "        'f_ever_bought_grp_fd3bfb', 'f_ever_bought_lh_e22a6a',\n",
    "        'f_ever_bought_grp_70e1dd', 'f_ever_bought_grp_e04c3a',\n",
    "        'f_ever_bought_grp_fe5fb8', 'f_ever_bought_grp_94baec',\n",
    "        'f_ever_bought_grp_e91421', 'f_ever_bought_lh_f852af',\n",
    "        'f_ever_bought_lh_947b15', 'f_ever_bought_32c74c', 'f_elx',\n",
    "        'f_mindef_mha', 'f_retail', 'flg_affconnect_show_interest_ever',\n",
    "        'flg_affconnect_ready_to_buy_ever', 'flg_affconnect_lapse_ever',\n",
    "        'flg_hlthclaim_839f8a_ever', 'recency_hlthclaim_839f8a',\n",
    "        'flg_hlthclaim_14cb37_ever', 'giclaim_cnt_success',\n",
    "        'recency_giclaim_success', 'giclaim_cnt_unsuccess',\n",
    "        'recency_giclaim_unsuccess', 'flg_gi_claim_29d435_ever',\n",
    "        'flg_gi_claim_058815_ever', 'flg_gi_claim_42e115_ever',\n",
    "        'flg_gi_claim_856320_ever'\n",
    "    ]\n",
    "    for col in columns_to_replace:\n",
    "        test_df[col] = test_df[col].fillna(test_df[col].mean())\n",
    "    test_df.drop(columns=['hh_size_est', \"ctrycode_desc\",'clntnum'], inplace=True)\n",
    "\n",
    "    if isTraining:\n",
    "        #Fill na for target column with 0s\n",
    "        test_df['f_purchase_lh']=test_df['f_purchase_lh'].fillna(0)\n",
    "\n",
    "    # Cleaning annual income col\n",
    "    columns = ['annual_income_est']\n",
    "    test_df['annual_income_est'] = test_df['annual_income_est'].fillna('None')\n",
    "\n",
    "    # Perform one-hot encoding\n",
    "    df_encoded = pd.get_dummies(test_df['annual_income_est'], dummy_na=True)\n",
    "\n",
    "    # Replace NaN values with 0 in the dummy columns\n",
    "    df_encoded_ = df_encoded.iloc[:, :-2]\n",
    "    test_df.drop('annual_income_est', axis=1, inplace=True)\n",
    "\n",
    "    result_df = pd.concat([test_df, df_encoded_], axis=1)\n",
    "\n",
    "    # Clean race\n",
    "    result_df['race_desc'] = result_df['race_desc'].fillna('Others')\n",
    "    df_dummies = pd.get_dummies(result_df['race_desc'], prefix='is_')\n",
    "\n",
    "    # Concatenate the dummy columns with the original DataFrame\n",
    "    df = pd.concat([result_df, df_dummies], axis=1)\n",
    "\n",
    "    # Drop the original categorical column if needed\n",
    "    df.drop('race_desc', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # Change dob to age by years\n",
    "    current_date = datetime.now()\n",
    "    mean_date = df[df['cltdob_fix'] != 'None']['cltdob_fix'].astype('datetime64[ns]').mean()\n",
    "    if isTraining:\n",
    "        df = df[df['cltdob_fix'] != 'None']\n",
    "    else:\n",
    "        df['cltdob_fix'] = df['cltdob_fix'].replace('None', mean_date)\n",
    "    df['cltdob_fix'] = (current_date - pd.to_datetime(df['cltdob_fix'])).dt.days // 365\n",
    "    df=df.rename(columns={'cltdob_fix': 'age'})\n",
    "\n",
    "    mean_date = df[df['min_occ_date'] != 'None']['min_occ_date'].astype('datetime64[ns]').mean()\n",
    "    # Change occ date to number of years\n",
    "    if isTraining:\n",
    "        df = df[df['min_occ_date'] != 'None']\n",
    "    else:\n",
    "        df['min_occ_date'] = df['min_occ_date'].replace('None', mean_date)\n",
    "    df['min_occ_date'] = (current_date - pd.to_datetime(df['min_occ_date'])).dt.days // 365\n",
    "    df=df.rename(columns={'min_occ_date': 'years_since_first_int'})\n",
    "\n",
    "    # Clean gender\n",
    "    df_dummy = pd.get_dummies(df['cltsex_fix'], prefix='is')\n",
    "\n",
    "\n",
    "    # Concatenate the dummy columns with the original DataFrame\n",
    "    df = pd.concat([df, df_dummy], axis=1)\n",
    "\n",
    "    # Drop the original categorical column if needed\n",
    "    df.drop('cltsex_fix', axis=1, inplace=True)\n",
    "    df[\"is_Female\"] = df[\"is_Female\"].astype(int)\n",
    "    df[\"is_Male\"] = df[\"is_Male\"].astype(int)\n",
    "    \n",
    "    # Clean Customer Status \n",
    "    df_dummy = pd.get_dummies(df['clttype'], prefix='is')\n",
    "    # Concatenate the dummy columns with the original DataFrame\n",
    "    df = pd.concat([df, df_dummy], axis=1)\n",
    "\n",
    "    # Drop the original categorical column if needed\n",
    "    df.drop('clttype', axis=1, inplace=True)\n",
    "    df[\"is_P\"] = df[\"is_P\"].astype(int)\n",
    "    df[\"is_G\"] = df[\"is_G\"].astype(int)\n",
    "    df[\"is_C\"] = df[\"is_C\"].astype(int)\n",
    "\n",
    "    # Clean stat_flag\n",
    "    df_dummy = pd.get_dummies(df['stat_flag'], prefix='is')\n",
    "    # Concatenate the dummy columns with the original DataFrame\n",
    "    df = pd.concat([df, df_dummy], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Drop the original categorical column if needed\n",
    "    df.drop('stat_flag', axis=1, inplace=True)\n",
    "    df[\"is_ACTIVE\"] = df[\"is_ACTIVE\"].astype(int)\n",
    "    df[\"is_LAPSED\"] = df[\"is_LAPSED\"].astype(int)\n",
    "    df[\"is_MATURED\"] = df[\"is_MATURED\"].astype(int)\n",
    "    #grouping all the APEs, SUMINs, PREMPAIDs together\n",
    "\n",
    "    p0 = [\"lapse_\", \"\"]\n",
    "    p1 = ['ape', 'sumins', 'prempaid']\n",
    "    types = ['_gi','_ltc', '_grp', '_inv', '_lh']\n",
    "    for k in p0:\n",
    "        for i in p1:\n",
    "            for j in types:\n",
    "                prefix = k + i + j\n",
    "                grp_cols = [col for col in df.columns if col.startswith(prefix)]\n",
    "                if grp_cols:\n",
    "                    avg = df[grp_cols].mean(axis=1)\n",
    "                    df = pd.concat([df, avg.rename(\"consolidated_\"+prefix)], axis=1)\n",
    "                    df.drop(columns=grp_cols, axis=1, inplace=True)\n",
    "\n",
    "    for i in p1:\n",
    "        grp_cols = [col for col in df.columns if col.startswith(i)]\n",
    "        df.drop(columns=grp_cols, inplace=True)\n",
    "\n",
    "    p1 = ['ltc', 'gi', 'lh', 'grp']\n",
    "    for i in p1:\n",
    "        prefix = \"f_ever_bought_\" + i\n",
    "        grp_cols = [col for col in df.columns if col.startswith(prefix)]\n",
    "        if grp_cols:\n",
    "            avg = df[grp_cols].mean(axis=1).fillna(0)\n",
    "            df = pd.concat([df, avg.rename(\"consolidated_\" + prefix )], axis=1)\n",
    "            df.drop(columns=grp_cols, axis=1, inplace=True)\n",
    "\n",
    "    grp_cols = [col for col in df.columns if col.startswith(\"f_ever_bought\")]\n",
    "    df.drop(columns=grp_cols, inplace=True)\n",
    "\n",
    "    p0 = ['last_bought_','since_lapse_']\n",
    "    p1 = ['ltc', 'gi', 'lh', 'grp', 'inv']\n",
    "    for i in p0:\n",
    "        for j in p1:\n",
    "            prefix = \"n_months_\" + i+ j\n",
    "            grp_cols = [col for col in df.columns if col.startswith(prefix)]\n",
    "            if grp_cols:\n",
    "                avg = df[grp_cols].mean(axis=1).fillna(0)\n",
    "                df = pd.concat([df, avg.rename(\"consolidated_\" + prefix )], axis=1)\n",
    "                df.drop(columns=grp_cols, axis=1, inplace=True)\n",
    "\n",
    "    grp_cols = [col for col in df.columns if col.startswith(\"n_months\")]\n",
    "    df.drop(columns=grp_cols, inplace=True)\n",
    "\n",
    "    thresh=0.9\n",
    "    for col in df.columns:\n",
    "        if (df[col].isna().sum() > 0).any():\n",
    "            if df[col].isna().sum()/df.shape[0]>thresh:\n",
    "                df.drop(columns=col,inplace=True)\n",
    "            else:\n",
    "                df[col]=df[col].astype(float)\n",
    "                df[col]=df[col].fillna(df[col].mean())\n",
    "        \n",
    "        elif df[col].dtype == object:\n",
    "            df[col]=df[col].astype(float)\n",
    "\n",
    "    # Minmaxscaler, apply to grp, inv and 1h\n",
    "    scaler=MinMaxScaler()\n",
    "    columns=['consolidated_prempaid_grp',\n",
    "            'consolidated_prempaid_grp',\n",
    "            'consolidated_prempaid_lh',\n",
    "            'consolidated_n_months_last_bought_ltc',\n",
    "            'consolidated_n_months_last_bought_gi',\n",
    "            'consolidated_n_months_last_bought_lh',\n",
    "            'consolidated_n_months_last_bought_grp',\n",
    "            'consolidated_n_months_last_bought_inv',\n",
    "            'consolidated_n_months_since_lapse_ltc',\n",
    "            'consolidated_n_months_since_lapse_lh',\n",
    "            'consolidated_n_months_since_lapse_grp',\n",
    "            'consolidated_n_months_since_lapse_inv',\n",
    "            'consolidated_ape_ltc',\n",
    "            'consolidated_ape_grp',\n",
    "            'consolidated_ape_inv',\n",
    "            'consolidated_ape_lh',\n",
    "            'consolidated_lapse_ape_ltc',\n",
    "            'consolidated_lapse_ape_grp',\n",
    "            'consolidated_lapse_ape_inv',\n",
    "            'consolidated_lapse_ape_lh',\n",
    "            'consolidated_sumins_ltc',\n",
    "            'consolidated_sumins_grp',\n",
    "            'consolidated_sumins_inv',\n",
    "            'consolidated_sumins_lh',\n",
    "            'age',\n",
    "            'hh_20',\n",
    "            'pop_20',\n",
    "            'hh_size'\n",
    "    ]\n",
    "    df[columns]=scaler.fit_transform(df[columns])\n",
    "\n",
    "    if isTraining:\n",
    "        # Dropping those with only 0 or 1 unique values\n",
    "        columns_to_drop1 = [col for col in df.columns if df[col].nunique() == 1 or df[col].nunique() == 0]\n",
    "        df = df.drop(columns=columns_to_drop1)\n",
    "    else:\n",
    "        df = df.fillna(df.mean())\n",
    "    return df\n",
    "\n",
    "# Oversampling, returns X_data, y_data\n",
    "def oversample_data(x_input,y_input):\n",
    "    over1=SMOTE(sampling_strategy='auto')\n",
    "    over2=RandomOverSampler(sampling_strategy=0.1)\n",
    "    steps=[('o7',over2),('o1',over1)]\n",
    "    pipeline=Pipeline(steps=steps)\n",
    "    X,y=pipeline.fit_resample(x_input,y_input)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif,chi2\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, pre_trained_model = None, selected_cols = None):\n",
    "        if pre_trained_model == None:\n",
    "            self.model = LinearSVC(random_state=42, C=0.028,class_weight='balanced')\n",
    "        else:\n",
    "            self.model = pre_trained_model\n",
    "        if selected_cols != None:\n",
    "            self.selected_col = selected_cols\n",
    "        else:\n",
    "            self.selected_col = None\n",
    "\n",
    "    def fit(self, input_data):\n",
    "        # Clean and oversample data\n",
    "        np.random.seed(42)\n",
    "        data = clean_data(input_data, True)\n",
    "        # Select best features\n",
    "        y=data['f_purchase_lh']\n",
    "        X=data.drop(columns='f_purchase_lh')\n",
    "        selector = SelectKBest(mutual_info_classif, k=16)\n",
    "        X_new = selector.fit_transform(X, y)\n",
    "        selected_columns_indices = selector.get_support(indices=True)\n",
    "        selected_columns = X.columns[selected_columns_indices]\n",
    "        print(selected_columns)\n",
    "        self.selected_col = selected_columns\n",
    "        X_new = pd.DataFrame(X, columns=selected_columns)\n",
    "        x_final,y_final = oversample_data(X_new,y)\n",
    "        self.model.fit(x_final, y_final)\n",
    "        \n",
    "\n",
    "    def predict(self, X_test):\n",
    "        data = clean_data(X_test, False)\n",
    "        data_to_test = data[self.selected_col]\n",
    "        pred_y = self.model.predict(data_to_test)\n",
    "        \n",
    "        return pred_y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING FOR BEST K\n",
    "\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# k_values = [15,16,17,18,19,20] #after testing lower bounds and higher bounds\n",
    "# cv_scores = []\n",
    "# for k in k_values:\n",
    "#     selector = SelectKBest(mutual_info_classif, k=k)\n",
    "#     X_new = selector.fit_transform(X,y)\n",
    "#     selected_columns_indices = selector.get_support(indices=True)\n",
    "#     selected_columns = X.columns[selected_columns_indices]\n",
    "#     print(selected_columns)\n",
    "#     X_new = pd.DataFrame(X, columns=selected_columns)\n",
    "#     xnew,ynew=pipeline.fit_resample(X_new,y)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(xnew, ynew,test_size=0.2)\n",
    "#     # Model\n",
    "#     model = LinearSVC(random_state=42, C=0.026)\n",
    "#     scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "#     mean_score = np.mean(scores)\n",
    "        \n",
    "#         # Append the mean score to the list of cross-validation scores\n",
    "#     cv_scores.append(mean_score)\n",
    "# # Find the value of k with the highest cross-validation score\n",
    "# best_k = k_values[np.argmax(cv_scores)]\n",
    "# print(best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUR FUNCTION FOR LEARNING CURVE \n",
    "\n",
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# train_sizes, train_scores, test_scores = learning_curve(\n",
    "#     model, xnew, ynew, cv=kfold, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "# )\n",
    "\n",
    "# train_scores = 1-np.mean(train_scores,axis=1)#converting the accuracy score to misclassification rate\n",
    "# test_scores = 1-np.mean(test_scores,axis=1)#converting the accuracy score to misclassification rate\n",
    "# plt.plot(train_sizes, train_scores, label='Training')\n",
    "# plt.plot(train_sizes, test_scores, label='Cross-validation')\n",
    "# plt.xlabel('Training Set Size')\n",
    "# plt.ylabel('Misclassification rate')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_parquet(filepath)\n",
    "X=test_df.drop(columns='f_purchase_lh')\n",
    "y=test_df['f_purchase_lh']\n",
    "y=y.fillna(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2) \n",
    "concatenated_train_data = pd.concat([X_train, y_train], axis=1)\n",
    "model = Model()\n",
    "model.fit(test_df)\n",
    "\n",
    "\n",
    "print(X_test.shape)\n",
    "pred_y = model.predict(X_test)\n",
    "\n",
    "\n",
    "y_test = y_test.fillna(0)\n",
    "\n",
    "f1_test = f1_score(y_test, pred_y)\n",
    "\n",
    "print('The f1 score for the testing data:', f1_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "confusion_matrix(y_test, pred_y)\n",
    "\n",
    "classification_rep = classification_report(y_test, pred_y)\n",
    "print('Classification Report:\\n', classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "def load_model(training_data):\n",
    "    pre_trained_model = joblib.load('model.joblib')\n",
    "    with open('selected_col.pkl', 'rb') as file:\n",
    "        selected_col = pickle.load(file)\n",
    "    model = Model(pre_trained_model)\n",
    "    model.fit(training_data)\n",
    "    \n",
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    final_result = model.predict(hidden_data)\n",
    "    result=final_result.tolist()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.read_parquet(filepath)\n",
    "test_df=test_df.drop(columns=['f_purchase_lh'])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
